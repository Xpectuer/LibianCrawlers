# LibianCrawler 系统概述

LibianCrawler 是一艘覆盖大数据开发全周期的航母。

其设计目标是为 复杂的、多来源 的数据获取场景提供一套标准化、可扩展的解决方案。

[[toc]]

## 功能概述

数据开发都要经历这些阶段，如下表所示:

| <div style="width:170px">阶段</div> | 传统框架                                   | 痛点                                                                       | 解决痛点的新架构                                                                                                                                                        |
|-----------------------------------|----------------------------------------|--------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 原始数据 采集                           | 各类脚本爬虫、pandas/polars 读取文件、第三方 API      | 代码启动的环境与参数很不统一，导致了稳定性、风控、协调的质量参差不齐。                                      | LibianCrawler 集成环境统一了传统框架的各种工具（如 各模块共用、参数归一化、确保指纹浏览器的 脚本-schema-MCP 接口一致性）。data_cleaner_ci 数据清洗环节除了能清洗数据沼泽外，也可以直接读取 **规整的数据结构**（如 nocodb、sqlite、csv）。           |
| 原始数据 结构化                          | 看数据采集的脚本作者心情                           | 存储方式八仙过海，因此结构化对象的关联混乱无比。                                                 | 统一丢给 MinIO，将其变为结构化的 url 类型，即 MinIO 的文件地址。                                                                                                                       |  
| 原始数据 被采集脚本处理                      | API爬虫-js逆向;html静态爬虫-xpath;html动态爬虫-都有; | 这几种模式都很怕数据结构不稳定。                                                         | data_cleaner_ci 先使用 jsonata 初步转换。随后对读取的数据 **生成 typescript 类型（Code Gen）**。                                                                                       |
| 原始数据 存储                           | 脚本爬虫通常写入 csv 和 sqlite                  | 在工程目录里乱拉屎，文件体积更是奇大也迫使 pandas/polars 不得不批次处理引发代码屎山危机。 原始数据的冗余经常由于各种原因被丢失。 | 在专用的 [postgres 数据沼泽](../develop/datalake/service-arch.md#数据沼泽) 里拉屎。postgres 的高效 jsonb 格式也非常节省存储空间，因此 **不丢失冗余** 也不会如何。                                           |                                                                       
| 清洗为 数据对象                          | pandas/polars 将不同的爬虫的不同的列对齐，将其数据格式归一化  | pandas/polars 无类型提示，开发非常耗费心智，在运行代码时遇到脏数据的 TypeError更是要命                  | data_cleaner_ci 生成 typescript 类型后，再在 **typescript 上下游环节类型检查下** 进行转换脚本开发。再也不会 TypeError                                                                          |
| 清洗为 合并数据对象                        | pandas/polars 很难做，一般八仙过海的会从旧数据集里做合并。   | 不能很优雅的去重和合并。 pandas/polars 的无类型提示，也加重了转换的心智负担。                           | data_cleaner_ci 将数据归为几类与数据湖的表格相对应的合并数据对象，同样在 **typescript 的类型体操和类型检查** 下确保正确性。然后根据 platform_duplicate_id 索引列开发去重合并代码，使每一条合并后数据都可以有完整的生命周期。                      |
| 清洗为 数据库对象                         | pandas/polars 将数据写入到新 csv 或某个数据库       | 难以管理数据库字段版本迁移。写入csv则又一次的在工程目录里乱拉屎。 pandas/polars 的无类型提示，也加重了转换的心智负担。     | 写入专用的 [postgres 数据仓库](../develop/datalake/service-arch.md#数据仓库) 。其表和字段的版本迁移使用 Kysely Migration 进行管理，且同样使用 **typescript 上下游环节类型检查** 确保 合并数据对象的列 to 数据库新旧字段 的正确性。 |                                              
| 数据分析                              | 使用 pandas/polars/R/excel 数据分析          | 需要 **会技术的数据分析师** 把甲方需求变成筛选条件。                                            | 使用 NocoDB 视图，让 **不懂技术的产品、运维、甲方自己** 也能开心玩耍筛选器。                                                                                                                   |                                            
| 数据展示                              | 使用 excel 展示                            | 版本管理麻烦，在工程目录里乱拉屎。                                                        | 分享 NocoDB 视图，新数据会自行更新在网页上。                                                                                                                                      |                                                 

[//]: # (系统的核心功能围绕着自动化数据工作流展开，具体包括以下几个方面：)

[//]: # ()

[//]: # (- **数据采集能力**: 系统支持两种主要的数据采集模式：)

[//]: # (    1. **API 直连采集**:  针对提供结构化数据接口（API）的目标源，系统能够直接发起网络请求以获取数据。)

[//]: # (    2. **浏览器自动化采集**: 针对需要动态渲染或用户交互的Web页面，系统利用  `Playwright`)

[//]: # (       库驱动 [Camoufox]&#40;https://camoufox.com/&#41; 指纹浏览器，模拟用户行为进行数据抓取。)

[//]: # ()

[//]: # (- **分布式任务执行**: 系统包含一个 `worker` 组件，用于任务的调度与执行。该设计允许将数据采集任务分发至不同的执行节点，从而支持并行处理，提高了大规模数据采集的吞吐量和系统伸缩性。)

[//]: # ()

[//]: # (- **声明式任务定义**: 数据采集流程（步骤、目标、参数等）被抽象为外部的 `JSON`)

[//]: # (  配置文件。这种声明式的方法将任务逻辑与执行引擎分离，使得非开发人员也能通过修改配置来调整或新增采集任务，降低了系统的使用门槛。)

[//]: # ()

[//]: # (- **数据处理流水线**: 系统包含一个独立的数据处理模块，该模块采用 `Deno` 作为 `TypeScript`)

[//]: # (  运行时环境。它负责对采集到的原始数据执行清洗、格式转换、结构化提取等操作。)

[//]: # ()

[//]: # (- **用户交互界面**: 系统提供一个基于 `Vue.js` 的Web前端应用，作为 `worker` 系统的图形化管理终端。该界面用于监控任务状态、查看结果和管理节点。使用)

[//]: # (  `pywebview` 库封装为桌面应用程序，实现了Python后端与前端UI的本地集成。)

[//]: # ()

[//]: # (:::details 技术架构)

[//]: # ()

[//]: # (LibianCrawler 采用多语言、多模块的微服务架构，旨在结合不同技术栈的优势，实现功能解耦和独立部署。)

[//]: # ()

[//]: # (- **核心后端 &#40;Python&#41;**:)

[//]: # (    - **语言与环境**: 使用 `Python` 作为后端服务的主要开发语言。依赖管理和虚拟环境隔离通过 `Poetry` 工具实现。)

[//]: # (    - **核心组件**: `libiancrawlers` 包是系统的核心，包含了 `worker` 任务调度器 &#40;`core.py`&#41; 和执行节点 &#40;`node.py`&#41; 的实现。)

[//]: # (      `app_util` 子模块则提供了配置加载、日志、网络请求封装等应用级支持功能。)

[//]: # (    - **浏览器驱动**: 通过 `playwright_util.py` 对 `Playwright` 库进行封装，为上层应用提供统一的浏览器操作接口。)

[//]: # ()

[//]: # (- **数据处理服务 &#40;TypeScript/Deno&#41;**:)

[//]: # (    - **运行时**: `data_cleaner_ci` 模块选用 `Deno` 作为其 `TypeScript` 代码的运行时环境，利用其内置的安全性、工具链和对Web标准API的支持。)

[//]: # (    - **数据转换**: 该服务大量使用 `JSONata` 查询和转换语言。预定义的 `.jsonata` 模板用于执行复杂嵌套数据结构的提取和重塑，实现了数据转换逻辑的可视化和集中管理。)

[//]: # (    - **数据持久化**: `nocodbutil.ts` 和 `pg.ts` 文件表明，数据处理结果可被写入 `PostgreSQL` 数据库，并可能通过 `NocoDB`)

[//]: # (      平台进行管理和访问。)

[//]: # ()

[//]: # (- **前端界面 &#40;Vue.js&#41;**:)

[//]: # (    - **框架与构建**: `worker-ui` 目录是一个标准化的 `Vue.js` 3 项目，使用 `TypeScript` 进行类型约束，并采用 `Vite`)

[//]: # (      作为开发服务器和构建工具，以实现快速开发和高效打包。)

[//]: # (    - **组件化**: 界面遵循组件化开发模式，`auto-imports.d.ts` 和 `components.d.ts` 表明项目使用了自动导入功能以简化开发。)

[//]: # (    - **后端集成**: `pywebview.d.ts` 类型定义文件是关键，它为前端代码提供了与 `pywebview`)

[//]: # (      注入的Python后端通信的接口类型，这是实现桌面应用功能的桥梁。)

[//]: # ()

[//]: # (:::)

[//]: # ()

[//]: # (:::details 项目结构解析)

[//]: # ()

[//]: # (项目的目录结构反映了其模块化的设计思想，各主要目录的职责划分如下：)

[//]: # ()

[//]: # (- `libiancrawlers/`: Python 核心后端代码库。)

[//]: # (    - `app_util/`: 应用层工具，如 `config.py` &#40;配置管理&#41;, `playwright_util.py` &#40;浏览器自动化封装&#41;。)

[//]: # (    - `crawlers/`: 存放具体爬虫逻辑的实现，可能按目标网站或类型分子目录。)

[//]: # (    - `worker/`: 分布式任务执行框架，包含 `core.py` &#40;核心调度&#41;, `node.py` &#40;工作节点&#41;, `ui.py` &#40;与UI的接口&#41;。)

[//]: # ()

[//]: # (- `data_cleaner_ci/`: TypeScript 数据处理服务代码库。)

[//]: # (    - `general_data_process/`: 通用数据处理脚本。)

[//]: # (    - `jsonata_templates/`: 存放 `.jsonata` 格式的数据转换模板。)

[//]: # ()

[//]: # (- `worker-ui/`: Vue.js 前端应用代码库。)

[//]: # ()

[//]: # (- `steps/`: 存放 `.json` 格式的预定义采集任务。每个文件（如`baidu.json`）描述了一个完整的数据采集流程。)

[//]: # ()

[//]: # (- `docs/`: 项目文档，使用 `VitePress` 静态站点生成器构建。)

[//]: # ()

[//]: # (:::)

[//]: # ()

[//]: # (## 工作流与数据流)

[//]: # ()

[//]: # (一个典型的数据处理任务在 LibianCrawler 系统中的生命周期如下：)

[//]: # ()

[//]: # (1. **任务启动**: 用户或外部系统通过 [**直接命令行启动**]&#40;../develop/crawler/start-crawl&#41; 或 调用 `worker` 服务的接口，并可指定一个位于)

[//]: # (   `steps/` 目录下的 `JSON` 任务配置文件来启动一个爬虫。)

[//]: # ()

[//]: # (2. **任务执行**: `libiancrawlers` 接收到请求，解析 `JSON` 文件中 [**定义的步骤)

[//]: # (   **]&#40;../develop/crawler/crawler-detail/steps.md&#41;，并调度相应的爬虫模块（API或浏览器）开始执行数据采集。)

[//]: # ()

[//]: # (3. **原始数据缓存**: 采集到的原始数据（如 HTML、JSON 响应）被存储于垃圾湖中，等待后续处理。)

[//]: # ()

[//]: # (4. **数据移交与清洗**: `libiancrawlers` 将原始数据移交给 `data_cleaner_ci` 服务。该服务根据预设的规则（如 `JSONata`)

[//]: # (   模板）对数据进行解析、验证、清洗和结构化。)

[//]: # ()

[//]: # (5. **数据输出**: 清洗后的结构化数据被写入目标存储，例如 PostgreSQL 数据库。)

[//]: # ()

[//]: # (6. **状态监控**: 在整个过程中，用户可以通过 `worker-ui` 界面实时监控任务的执行状态、进度和结果。)

[//]: # ()

## 开发与贡献

[//]: # (- **项目蓝图**: [**`docs/develop/roadmap.md`**]&#40;../develop/roadmap&#41; 文件中可能包含了项目未来的发展方向和功能规划，为潜在的贡献者提供了参与的思路。)

