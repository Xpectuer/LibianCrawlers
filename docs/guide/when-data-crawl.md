# 2-数据采集中的挑战与解决方案

在数据采集过程中，无论是通过 API 还是浏览器自动化，开发者都可能面临一系列复杂的问题。LibianCrawler 针对这些挑战提供了更加稳健和灵活的解决方案。

## 基于 API 的数据采集

当目标平台的 API 没有复杂的加密机制（如 sign 签名算法），或者已经有成熟的 API SDK 可以直接使用时，数据采集工作会变得非常顺畅。

在这种情况下，传统方案能够高效完成数据采集任务。

LibianCrawler 为 API 采集封装了常用功能，例如:

- 在采集阶段，封装了参数相似的高层 API，提供了通用的工具库。
- 在清洗阶段，数据清洗 CI 依然可以为一些“面向dict编程”的API提供强类型。并分门别类的输出清洗后的列对齐结果。

[//]: # ([//]: # &#40;- 提供分布式爬虫的上下文存储设施，避免重复爬取。&#41;)
[//]: # ()
[//]: # ([//]: # &#40;- 支持任务的暂停与恢复，提高采集的灵活性。&#41;)
[//]: # ()
[//]: # ([//]: # &#40;- 封装了常用平台的 API 库，提供简洁的命令行接口，便于任务调度。&#41;)
[//]: # (:::)

## 基于浏览器自动化的数据采集

当 API 存在复杂的 sign 签名算法时，传统方案通常会选择逆向这些算法。
这是一项极其繁琐且耗时的工作，因为开发者需要与一个甚至数十个平台方的安全团队展开“军备竞赛”。

因此，LibianCrawler 采用了更为保护个人开发者头发总量的方案 —— 直接从浏览器 Frames DOM 树中获取内容，避免逆向复杂算法。

尽管浏览器自动化技术已经非常成熟，但在实际应用中仍然面临以下挑战：

### DOM 元素的提取

传统方案通常直接使用 XPath 或选择器从 DOM 中提取特定元素，将采集和清洗步骤合为一体。这种方法存在一个致命问题：

- **对网页结构高度依赖**：当目标网站更新其页面布局时，采集程序可能会完全失效。

LibianCrawler 的解决方案：

- 在采集阶段，只负责将所有内容（如 DOM、API 响应）保存下来，而不直接提取特定字段。
- 在清洗阶段，专门处理数据提取工作。当网页更新时，只需修改清洗逻辑，而采集程序可以继续运行。

这种方法的优势：

- 网页更新时，采集程序不需要停止。
- 清洗程序会通过类型检查自动检测结构变化。如果业务代码仍可复用，则无需调整；否则会停止清洗以待修复。

### 处理弹出式验证

传统方案和 LibianCrawler 在处理弹出式验证时采用了相似的方法，即:

- 检测到特定情形时，暂停采集程序并弹出对话框和通知，等待程序员手动处理。

但 LibianCrawler 在未来可能会开发更完善的分布式解决方案。

### 防止反爬虫指纹检测

LibianCrawler 使用 [Camoufox](https://camoufox.com/) 规避浏览器指纹检测。

:::tip Why Camoufox
[Camoufox](https://camoufox.com/) 旨在成为一款具有强大指纹注入和反机器人逃避功能的简约浏览器。
:::

#### 在 API 采集时同样修改指纹

在使用 API 采集数据时，LibianCrawler 会尽可能的 Hook API 库中的 http 请求，并挑选 proxy、tls 指纹、header，**但是这项功能尚未开发完成**。

#### 统一 proxy,geo,font,locale

LibianCrawler 还会维护 代理池+指纹库，并确保 geoip、proxy-ip、locale、font 等环境参数相互印证，**但是这项功能尚未开发完成**。

Locale 设置也应当传给数据清洗 CI 以便在清洗时提供当地的日期，**但是这项功能尚未开发完成**。

## 总结

在数据采集领域，LibianCrawler 通过以下方式显著优化了传统解决方案：

- 避免复杂逆向工作：直接从浏览器环境中获取数据。
- 分离采集与清洗：减少对目标平台结构变化的敏感度。
- 灵活性和扩展性：支持分布式任务调度、多种存储后端以及自定义反反爬虫策略。

通过这些特性，LibianCrawler 能够显著提高数据采集的稳定性和效率，同时降低维护成本。
