import{_ as a,c as r,o as d,a3 as e}from"./chunks/framework.w8luOYmq.js";const b=JSON.parse('{"title":"LibianCrawler 系统概述","description":"","frontmatter":{},"headers":[],"relativePath":"guide/learn-about-libiancrawler.md","filePath":"guide/learn-about-libiancrawler.md","lastUpdated":1762676459000}'),l={name:"guide/learn-about-libiancrawler.md"};function n(s,t,o,i,p,c){return d(),r("div",null,t[0]||(t[0]=[e('<h1 id="libiancrawler-系统概述" tabindex="-1">LibianCrawler 系统概述 <a class="header-anchor" href="#libiancrawler-系统概述" aria-label="Permalink to &quot;LibianCrawler 系统概述&quot;">​</a></h1><p>LibianCrawler 是一艘覆盖大数据开发全周期的航母。</p><p>其设计目标是为 复杂的、多来源 的数据获取场景提供一套标准化、可扩展的解决方案。</p><nav class="table-of-contents"><ul><li><a href="#libiancrawler-系统概述">LibianCrawler 系统概述</a><ul><li><a href="#功能概述">功能概述</a></li><li><a href="#开发与贡献">开发与贡献</a></li></ul></li></ul></nav><h2 id="功能概述" tabindex="-1">功能概述 <a class="header-anchor" href="#功能概述" aria-label="Permalink to &quot;功能概述&quot;">​</a></h2><p>数据开发都要经历这些阶段，如下表所示:</p><table tabindex="0"><thead><tr><th><div style="width:170px;">阶段</div></th><th>传统框架</th><th>痛点</th><th>解决痛点的新架构</th></tr></thead><tbody><tr><td>原始数据 采集</td><td>各类脚本爬虫、pandas/polars 读取文件、第三方 API</td><td>代码启动的环境与参数很不统一，导致了稳定性、风控、协调的质量参差不齐。</td><td>LibianCrawler 集成环境统一了传统框架的各种工具初始化接口。data_cleaner_ci 数据清洗环节也可以进行规整的数据结构的读取，比如读取 nocodb、sqlite、csv。</td></tr><tr><td>原始数据 结构化</td><td>看数据采集的脚本作者心情</td><td>存储方式八仙过海，因此结构化对象的关联混乱无比。</td><td>统一丢给 MinIO，将其变为结构化的 url 类型，即 MinIO 的文件地址。</td></tr><tr><td>原始数据 被采集脚本处理</td><td>API爬虫-js逆向;html静态爬虫-xpath;html动态爬虫-都有;</td><td>这几种模式都很怕数据结构不稳定。</td><td>data_cleaner_ci 先使用 jsonata 初步转换。随后对读取的数据 <strong>生成 typescript 类型（Code Gen）</strong>。</td></tr><tr><td>原始数据 存储</td><td>脚本爬虫通常写入 csv 和 sqlite</td><td>在工程目录里乱拉屎，文件体积更是奇大也迫使 pandas/polars 不得不批次处理引发代码屎山危机。 原始数据的冗余经常由于各种原因被丢失。</td><td>在专用的 postgres <strong>数据沼泽</strong> 里拉屎。postgres 的高效 jsonb 格式也非常节省存储空间。</td></tr><tr><td>清洗为 数据对象</td><td>pandas/polars 将不同的爬虫的不同的列对齐，将其数据格式归一化</td><td>pandas/polars 无类型提示，开发非常耗费心智，在运行代码时遇到脏数据的 TypeError更是要命</td><td>data_cleaner_ci 生成 typescript 类型后，再在 <strong>typescript 上下游环节类型检查下</strong> 进行转换脚本开发。再也不会 TypeError</td></tr><tr><td>清洗为 合并数据对象</td><td>pandas/polars 很难做，一般八仙过海的会从旧数据集里做合并。</td><td>不能很优雅的去重和合并。 pandas/polars 的无类型提示，也加重了转换的心智负担。</td><td>data_cleaner_ci 将数据归为几类与数据湖的表格相对应的合并数据对象，同样在 <strong>typescript 的类型体操和类型检查</strong> 下确保正确性。然后根据 platform_duplicate_id 索引列开发去重合并代码，使每一条合并后数据都可以有完整的生命周期。</td></tr><tr><td>清洗为 数据库对象</td><td>pandas/polars 将数据写入到新 csv 或某个数据库</td><td>难以管理数据库字段版本迁移。写入csv则又一次的在工程目录里乱拉屎。 pandas/polars 的无类型提示，也加重了转换的心智负担。</td><td>写入专用的 postgres <strong>数据湖</strong> 。其表和字段的版本迁移使用 Kysely Migration 进行管理，且同样使用 <strong>typescript 上下游环节类型检查</strong> 确保 数据对象 to 数据库新字段 的正确性。</td></tr><tr><td>数据分析</td><td>使用 pandas/polars/R/excel 数据分析</td><td>需要 <strong>会技术的数据分析师</strong> 把甲方需求变成筛选条件。</td><td>使用 NocoDB 视图，让 <strong>不懂技术的产品、运维、甲方自己</strong> 也能开心玩耍筛选器。</td></tr><tr><td>数据展示</td><td>使用 excel 展示</td><td>版本管理麻烦，在工程目录里乱拉屎。</td><td>分享 NocoDB 视图，新数据会自行更新在网页上。</td></tr></tbody></table><h2 id="开发与贡献" tabindex="-1">开发与贡献 <a class="header-anchor" href="#开发与贡献" aria-label="Permalink to &quot;开发与贡献&quot;">​</a></h2>',8)]))}const _=a(l,[["render",n]]);export{b as __pageData,_ as default};
