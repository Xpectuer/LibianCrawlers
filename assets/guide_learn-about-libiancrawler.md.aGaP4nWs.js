import{_ as r,c as e,o as a,ag as d}from"./chunks/framework.B3HI_LuJ.js";const b=JSON.parse('{"title":"了解 LibianCrawler","description":"","frontmatter":{},"headers":[],"relativePath":"guide/learn-about-libiancrawler.md","filePath":"guide/learn-about-libiancrawler.md","lastUpdated":1741761881000}'),l={name:"guide/learn-about-libiancrawler.md"};function n(o,t,i,s,p,c){return a(),e("div",null,t[0]||(t[0]=[d('<h1 id="了解-libiancrawler" tabindex="-1">了解 LibianCrawler <a class="header-anchor" href="#了解-libiancrawler" aria-label="Permalink to &quot;了解 LibianCrawler&quot;">​</a></h1><p>LibianCrawler 通过整合一系列工具， 构建了一个模块化的、易于维护的 Web 爬虫与数据清洗框架。</p><p><strong>其架构设计有效解决了传统工具在处理复杂网页、动态内容、接口变化等方面的诸多难题</strong>， 使开发者能够更专注于业务逻辑的实现，而非繁琐的数据抓取和清洗工作。</p><table tabindex="0"><thead><tr><th>在 Web 爬虫和数据清洗时的困难之处</th><th>playwright</th><th>typescript + jsonata + quicktype</th></tr></thead><tbody><tr><td>接口难以定位与逆向</td><td>利用 <a href="https://playwright.dev/" target="_blank" rel="noreferrer">Playwright</a> 记录请求和响应、HTML frame 树。</td><td>结合 TypeScript 和 Jsonata 的高效数据提取能力，解析并生成相应的类型定义。</td></tr><tr><td>指纹检测</td><td>使用 <a href="https://camoufox.com/" target="_blank" rel="noreferrer">Camoufox</a> 规避浏览器指纹检测。</td><td></td></tr><tr><td>弹出式验证</td><td>暂停脚本等待用户手动登陆、手动输入验证码。</td><td></td></tr><tr><td>平台接口变动、sign 升级</td><td>HTML frame 树并不会发生很大变化</td><td>利用 quicktype 生成返回值类型，然后利用 typescript 类型系统对业务代码类型检查。</td></tr></tbody></table><p><strong>在完成数据爬取和类型生成后</strong>，通过编写 <a href="https://www.typescriptlang.org/" target="_blank" rel="noreferrer">TypeScript</a> 业务代码，可以高效地完成数据清洗、验证和合并的任务。具体来说：</p><ul><li>确保新生成的类型与老代码兼容，避免类型推断检查失败。</li><li>对时间格式、数值范围等关键字段进行严格的验证和清洗，确保数据质量。</li><li>在合并数据时，灵活处理相同 id 的情况，确保去重或更新逻辑正确。</li></ul><p>诚然，尽管类型检查能够确保数据类型在变更过程中的兼容性，但如果原始数据的类型不整洁，则需要预先进行数据转换处理。此时，作为 JSON <strong>查询和转换</strong>语言的 <a href="https://jsonata.org/" target="_blank" rel="noreferrer">Jsonata</a> 将派上用场：它会在保留原始数据的基础上生成新的字段。而这些新字段也将在后续的 <a href="https://quicktype.io/" target="_blank" rel="noreferrer">Quicktype</a> <strong>类型生成</strong>中被输入，这样就可以让生成的类型保持整洁。</p><p><strong>在完成数据清洗之后</strong>，会使用 <a href="https://kysely.dev/" target="_blank" rel="noreferrer">Kysely</a> 框架将数据更新到 PostgreSQL 中的各个用于存放清洗后的数据的表中，这会根据以下规则进行操作：</p><ul><li>当数据库中不存在新数据的 id 时：插入一条新记录。</li><li>当数据库中存在旧数据但与新数据不一致时：更新旧数据为新数据。</li><li>当旧数据和新数据一致时：不会发生任何更改。</li></ul><p><strong>当面对新的数据清洗需求时</strong>，若需新建结果表或在现有表中添加新列，<a href="https://kysely.dev/docs/migrations" target="_blank" rel="noreferrer">Kysely Migrations</a> 提供了高效且可靠的方式来进行数据库迁移。这种方法不仅确保了代码变更与数据库模式的同步更新，还通过 TypeScript 实现类型安全，确保了 ORM 对象与数据库列之间的类型一致性。</p><p><strong>对比 LibianCrawler 的数据清洗方案和 pandas/polars 的数据清洗方案</strong></p><table tabindex="0"><thead><tr><th>特性</th><th>LibianCrawler</th><th>Pandas/Polars</th></tr></thead><tbody><tr><td>类型安全</td><td>利用 Quicktype 对 原始数据+jsonata 转换后的数据 生成类型定义代码，利用 Typescript 强类型语言做静态检查——这可以轻松发现 type error。</td><td>动态类型，依赖外部工具（如 Pydantic），容易在处理到一半时发生 type error ！<em>（在你大叫“jupyter 启动”然后等待 5 分钟前功尽弃，因为发现一个数字类型的列里居然有“1 万”、或是神秘的 NaN、或是 crlf 和字符集问题）</em></td></tr><tr><td>性能</td><td>中，适合中小规模数据清洗</td><td>高，尤其是 Polars 在大规模数据处理中表现优异</td></tr><tr><td>学习成本</td><td>中等，需要熟悉 TypeScript ，VSCode 会提示。Jsonata 不难而且可以渐进式学习。</td><td>中等，我很讨厌 Jupyter 这种没有好用的提示的半成品。而且 Pandas API 的面向魔术方法编程令人烦躁（但 Polars 还不错）。</td></tr><tr><td>数据合并&amp;存储&amp;筛选和展示</td><td>编程处理数据合并；实现了数据库数据增量更新；配合 <a href="https://nocodb.com/" target="_blank" rel="noreferrer">NocoDB</a> 可以给预览者提供开箱即用的筛选器和各类视图。</td><td>这些方面都没有提供很好的方案。在数据筛选时容易在项目目录中产生数个体积巨大的清洗结果文件——而且体积巨大的 excel 有严重卡顿，不便展示。</td></tr><tr><td>适用场景</td><td>适合 类型未知或杂乱、有增量数据、控制流复杂的数据清洗。适合给想法很多又很闲又不懂技术的甲方提供数据筛选和展示。</td><td>适合类型固定、无增量数据、控制流简单的数据清洗。适合懂技术的人群（实验室里的老登）自己动手的数据筛选和分析。</td></tr></tbody></table>',12)]))}const g=r(l,[["render",n]]);export{b as __pageData,g as default};
