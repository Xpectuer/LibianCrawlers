import{_ as o,c as l,o as r,a3 as c}from"./chunks/framework.w8luOYmq.js";const u=JSON.parse('{"title":"LibianCrawler 系统概述","description":"","frontmatter":{},"headers":[],"relativePath":"guide/learn-about-libiancrawler.md","filePath":"guide/learn-about-libiancrawler.md","lastUpdated":1755446592000}'),a={name:"guide/learn-about-libiancrawler.md"};function i(d,e,t,s,n,p){return r(),l("div",null,e[0]||(e[0]=[c('<h1 id="libiancrawler-系统概述" tabindex="-1">LibianCrawler 系统概述 <a class="header-anchor" href="#libiancrawler-系统概述" aria-label="Permalink to &quot;LibianCrawler 系统概述&quot;">​</a></h1><p>LibianCrawler 是一个集成了数据采集、处理、管理与可选用户界面的复合型系统。其设计目标是为复杂的、多来源的数据获取场景提供一套标准化、可扩展的解决方案。</p><nav class="table-of-contents"><ul><li><a href="#libiancrawler-系统概述">LibianCrawler 系统概述</a><ul><li><a href="#功能概述">功能概述</a></li><li><a href="#工作流与数据流">工作流与数据流</a></li><li><a href="#开发与贡献">开发与贡献</a></li></ul></li></ul></nav><h2 id="功能概述" tabindex="-1">功能概述 <a class="header-anchor" href="#功能概述" aria-label="Permalink to &quot;功能概述&quot;">​</a></h2><p>系统的核心功能围绕着自动化数据工作流展开，具体包括以下几个方面：</p><ul><li><p><strong>数据采集能力</strong>: 系统支持两种主要的数据采集模式：</p><ol><li><strong>API 直连采集</strong>: 针对提供结构化数据接口（API）的目标源，系统能够直接发起网络请求以获取数据。</li><li><strong>浏览器自动化采集</strong>: 针对需要动态渲染或用户交互的Web页面，系统利用 <code>Playwright</code> 库驱动 <a href="https://camoufox.com/" target="_blank" rel="noreferrer">Camoufox</a> 指纹浏览器，模拟用户行为进行数据抓取。</li></ol></li><li><p><strong>分布式任务执行</strong>: 系统包含一个 <code>worker</code> 组件，用于任务的调度与执行。该设计允许将数据采集任务分发至不同的执行节点，从而支持并行处理，提高了大规模数据采集的吞吐量和系统伸缩性。</p></li><li><p><strong>声明式任务定义</strong>: 数据采集流程（步骤、目标、参数等）被抽象为外部的 <code>JSON</code> 配置文件。这种声明式的方法将任务逻辑与执行引擎分离，使得非开发人员也能通过修改配置来调整或新增采集任务，降低了系统的使用门槛。</p></li><li><p><strong>数据处理流水线</strong>: 系统包含一个独立的数据处理模块，该模块采用 <code>Deno</code> 作为 <code>TypeScript</code> 运行时环境。它负责对采集到的原始数据执行清洗、格式转换、结构化提取等操作。</p></li><li><p><strong>用户交互界面</strong>: 系统提供一个基于 <code>Vue.js</code> 的Web前端应用，作为 <code>worker</code> 系统的图形化管理终端。该界面用于监控任务状态、查看结果和管理节点。使用 <code>pywebview</code> 库封装为桌面应用程序，实现了Python后端与前端UI的本地集成。</p></li></ul><details class="details custom-block"><summary>技术架构</summary><p>LibianCrawler 采用多语言、多模块的微服务架构，旨在结合不同技术栈的优势，实现功能解耦和独立部署。</p><ul><li><p><strong>核心后端 (Python)</strong>:</p><ul><li><strong>语言与环境</strong>: 使用 <code>Python</code> 作为后端服务的主要开发语言。依赖管理和虚拟环境隔离通过 <code>Poetry</code> 工具实现。</li><li><strong>核心组件</strong>: <code>libiancrawlers</code> 包是系统的核心，包含了 <code>worker</code> 任务调度器 (<code>core.py</code>) 和执行节点 (<code>node.py</code>) 的实现。<code>app_util</code> 子模块则提供了配置加载、日志、网络请求封装等应用级支持功能。</li><li><strong>浏览器驱动</strong>: 通过 <code>playwright_util.py</code> 对 <code>Playwright</code> 库进行封装，为上层应用提供统一的浏览器操作接口。</li></ul></li><li><p><strong>数据处理服务 (TypeScript/Deno)</strong>:</p><ul><li><strong>运行时</strong>: <code>data_cleaner_ci</code> 模块选用 <code>Deno</code> 作为其 <code>TypeScript</code> 代码的运行时环境，利用其内置的安全性、工具链和对Web标准API的支持。</li><li><strong>数据转换</strong>: 该服务大量使用 <code>JSONata</code> 查询和转换语言。预定义的 <code>.jsonata</code> 模板用于执行复杂嵌套数据结构的提取和重塑，实现了数据转换逻辑的可视化和集中管理。</li><li><strong>数据持久化</strong>: <code>nocodbutil.ts</code> 和 <code>pg.ts</code> 文件表明，数据处理结果可被写入 <code>PostgreSQL</code> 数据库，并可能通过 <code>NocoDB</code> 平台进行管理和访问。</li></ul></li><li><p><strong>前端界面 (Vue.js)</strong>:</p><ul><li><strong>框架与构建</strong>: <code>worker-ui</code> 目录是一个标准化的 <code>Vue.js</code> 3 项目，使用 <code>TypeScript</code> 进行类型约束，并采用 <code>Vite</code> 作为开发服务器和构建工具，以实现快速开发和高效打包。</li><li><strong>组件化</strong>: 界面遵循组件化开发模式，<code>auto-imports.d.ts</code> 和 <code>components.d.ts</code> 表明项目使用了自动导入功能以简化开发。</li><li><strong>后端集成</strong>: <code>pywebview.d.ts</code> 类型定义文件是关键，它为前端代码提供了与 <code>pywebview</code> 注入的Python后端通信的接口类型，这是实现桌面应用功能的桥梁。</li></ul></li></ul></details><details class="details custom-block"><summary>项目结构解析</summary><p>项目的目录结构反映了其模块化的设计思想，各主要目录的职责划分如下：</p><ul><li><p><code>libiancrawlers/</code>: Python 核心后端代码库。</p><ul><li><code>app_util/</code>: 应用层工具，如 <code>config.py</code> (配置管理), <code>playwright_util.py</code> (浏览器自动化封装)。</li><li><code>crawlers/</code>: 存放具体爬虫逻辑的实现，可能按目标网站或类型分子目录。</li><li><code>worker/</code>: 分布式任务执行框架，包含 <code>core.py</code> (核心调度), <code>node.py</code> (工作节点), <code>ui.py</code> (与UI的接口)。</li></ul></li><li><p><code>data_cleaner_ci/</code>: TypeScript 数据处理服务代码库。</p><ul><li><code>general_data_process/</code>: 通用数据处理脚本。</li><li><code>jsonata_templates/</code>: 存放 <code>.jsonata</code> 格式的数据转换模板。</li></ul></li><li><p><code>worker-ui/</code>: Vue.js 前端应用代码库。</p></li><li><p><code>steps/</code>: 存放 <code>.json</code> 格式的预定义采集任务。每个文件（如<code>baidu.json</code>）描述了一个完整的数据采集流程。</p></li><li><p><code>docs/</code>: 项目文档，使用 <code>VitePress</code> 静态站点生成器构建。</p></li></ul></details><h2 id="工作流与数据流" tabindex="-1">工作流与数据流 <a class="header-anchor" href="#工作流与数据流" aria-label="Permalink to &quot;工作流与数据流&quot;">​</a></h2><p>一个典型的数据处理任务在 LibianCrawler 系统中的生命周期如下：</p><ol><li><p><strong>任务启动</strong>: 用户或外部系统通过 <a href="./../develop/crawler/start-crawl.html"><strong>直接命令行启动</strong></a> 或 调用 <code>worker</code> 服务的接口，并可指定一个位于 <code>steps/</code> 目录下的 <code>JSON</code> 任务配置文件来启动一个爬虫。</p></li><li><p><strong>任务执行</strong>: <code>libiancrawlers</code> 接收到请求，解析 <code>JSON</code> 文件中 <a href="./../develop/crawler/steps.html"><strong>定义的步骤</strong></a>，并调度相应的爬虫模块（API或浏览器）开始执行数据采集。</p></li><li><p><strong>原始数据缓存</strong>: 采集到的原始数据（如 HTML、JSON 响应）被存储于垃圾湖中，等待后续处理。</p></li><li><p><strong>数据移交与清洗</strong>: <code>libiancrawlers</code> 将原始数据移交给 <code>data_cleaner_ci</code> 服务。该服务根据预设的规则（如 <code>JSONata</code> 模板）对数据进行解析、验证、清洗和结构化。</p></li><li><p><strong>数据输出</strong>: 清洗后的结构化数据被写入目标存储，例如 PostgreSQL 数据库。</p></li><li><p><strong>状态监控</strong>: 在整个过程中，用户可以通过 <code>worker-ui</code> 界面实时监控任务的执行状态、进度和结果。</p></li></ol><h2 id="开发与贡献" tabindex="-1">开发与贡献 <a class="header-anchor" href="#开发与贡献" aria-label="Permalink to &quot;开发与贡献&quot;">​</a></h2><ul><li><strong>项目蓝图</strong>: <a href="./../develop/roadmap.html"><strong><code>docs/develop/roadmap.md</code></strong></a> 文件中可能包含了项目未来的发展方向和功能规划，为潜在的贡献者提供了参与的思路。</li></ul>',13)]))}const h=o(a,[["render",i]]);export{u as __pageData,h as default};
